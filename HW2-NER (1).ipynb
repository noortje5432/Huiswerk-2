{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Second Group Assignment: A Named Entity Recognizer for Dutch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "TableOfContents": 1,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "[Introduction](#Introduction)  \n",
    "[0. Preparation: Training data](#0.-Preparation:-Training-data)  \n",
    "[1. Step 1: A minimal NER tagger for Dutch](#1.-Step-1:-A-minimal-NER-tagger-for-Dutch)   \n",
    "[2. Feature Extractors](#2.-Feature-Extractors)  \n",
    "[3. Step 2: Turn it into a program](#3.-Step-2:-Turn-it-into-a-program)  \n",
    "[4. Pickling and unpickling successfully](#3.-Pickling-and-unpickling-successfully)  \n",
    "[5. Self-testing](#4.-Self-testing)  \n",
    "[6. Step 3: Write scripts to build and evaluate your models](#6.-Step-3:-Write-scripts-to-build-and-evaluate-your-models)  \n",
    "[7. Step 4: Improve the model by improving the feature selection](#7.-Step-4:-Improve-the-model-by-improving-the-feature-selection)  \n",
    "[8. Step 5: Performance evaluation](#8.-Step-5:-Performance-evaluation)\n",
    "[9. Step 6: Report](#9.-Step-6:-Report)\n",
    "[10. Step 7: Submission](#10.-Step-7:-Submission)\n",
    "[11. Practicalities](#11.-Practicalities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "* NLTK book, Chapter 6: Classification and classifiers\n",
    "* NLTK book, Chapter 7: Chunking and named entity recognition\n",
    "* Jurafsky and Martin, ch. 8, cover some NER and other sequence-tagging basics\n",
    "* Jurafsky and Martin, ch. 17, give a high-level introduction to information extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The goal of this activity is to construct a Named Entity Recognizer\n",
    "(NER): A device that can scan natural text, identify named entities\n",
    "such as persons, places and organizations that are referred to by\n",
    "name, and classify them according to type (PERSON, LOCATION, etc.)\n",
    "\n",
    "You will train a classifier on the Dutch component of the CONLL2002\n",
    "corpus. (The corpus also includes a Spanish\n",
    "component, so always specify which files you want to read.)\n",
    "\n",
    "The necessary background concepts and software techniques are\n",
    "presented in chapter 7 of the NLTK book:\n",
    "\n",
    "[Section 7.2]: http://www.nltk.org/book/ch07.html#sec-chunking\n",
    "[Section 7.3]: http://www.nltk.org/book/ch07.html#developing-and-evaluating-chunkers\n",
    "[Section 7.5]: http://www.nltk.org/book/ch07.html#named-entity-recognition\n",
    "\n",
    "* [Section 7.2][] presents the concept of *chunking*, and how the NLTK\n",
    "manages its chunked corpora.\n",
    "\n",
    "* [Section 7.3][] shows how to build and evaluate chunkers with the\n",
    "help of the NLTK's chunked corpora. The discussion is based on the\n",
    "CONLL2000 corpus (note the year), a corpus of _English_ text in which\n",
    "all noun phrases are indicated.\n",
    "\n",
    "* Finally, [Section 7.5][] briefly covers the task of Named Entity\n",
    "Recognition. (Tip for the impatient: Sections 7.2 and 7.3 are\n",
    "essential reading--do not skip them).\n",
    "\n",
    "In the CONLL2002 corpus, which contains Spanish and Dutch components,\n",
    "only named entities have been chunked. Although the content of the\n",
    "chunks is different, the structure and interface of the corpora is the\n",
    "same: The text is annotated with POS tags, chunks, and chunk types.\n",
    "Thus the procedures for chunking noun phrases can be adapted to the NER task with\n",
    "minimal changes: Just train on the Dutch CONLL2002 corpus, and\n",
    "recognize _its_ chunks.\n",
    "\n",
    "Once you have trained a NER model, you will save it in a _pickle_ so that you can use it later without re-training it. Pickling is explained in section #TODO below.\n",
    "\n",
    "\n",
    "* Practicum 10 (Week 6) will explain a lot of what you need to know to do this assignment. It is made available now in case you want to get started on it. You can get started right away by reading the text book.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "**Note:** The nltk's classifiers and taggers need the external `numpy`\n",
    "library, but fail in a very confusing way if it is not found. The\n",
    "Anaconda distribution includes `numpy`, so that's not a problem unless\n",
    "you are using python without Anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Preparation: Training data\n",
    "\n",
    "We will once again use the `CONLL2002` corpus, which was specifically created for the task of named entity recognition.\n",
    "\n",
    "In some earlier practica, we split the file `ned.train` into training\n",
    "and testing components. In fact, the corpus includes\n",
    "separate datasets for testing. Use all of the file `\"ned.train\"` (and nothing else) to train your models. Use the file `\"ned.testa\"` for testing. (If you're familiar with machine learning vocabulary, this means you should use `testa` as your development set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Step 1: A minimal NER tagger for Dutch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**In brief:** Complete the wrapper `custom_chunker.py`, provided below. Train a named entity recognizer (NER) for Dutch. Pickle it and measure its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Section 7.3.3](http://www.nltk.org/book/ch07.html#training-classifier-based-chunkers) of the NLTK book provides sample code for a\n",
    "chunker, showing how to wrap a sequential MaxEnt classifier in a\n",
    "converter that acccepts chunked sentences in `Tree` format.\n",
    "You will write methods to complete module based on it. You'll save it as a module `custom_chunker.py`,\n",
    "and import it into your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "In the following steps you will write a function to prepare the training data, which you will then incorporate into the `_ConsecutiveNPChunkTagger` class below. \n",
    "\n",
    "To begin with, we need to import some things from `nltk` and check that we have `numpy`. If this raises an error, install `numpy` before you go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chunk.util import conlltags2tree, tree2conlltags\n",
    "\n",
    "# If numpy is absent, the nltk fails with a very confusing error.\n",
    "# We avoid problems by checking directly\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    print(\"You need to download and install numpy!!!\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data set is imported below. For debugging, use a tiny sample so you don't have to wait for anything to train. When you train your real models, use the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2002 as conll\n",
    "\n",
    "# for debugging, use a tiny corpus\n",
    "tiny_sample = 100\n",
    "training = conll.chunked_sents(\"ned.train\")[:tiny_sample] # SHORT DATASET: FOR DEMO/DEBUGGING ONLY! \n",
    "# training = conll.chunked_sents(\"ned.train\")\n",
    "testing = conll.chunked_sents(\"ned.testa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To use this module, you need a training dataset of chunked sentences\n",
    "(in `nltk.Tree` format) and a feature extractor function that will be used\n",
    "internally during training and regular use.\n",
    "\n",
    "## 2. Feature Extractors\n",
    "\n",
    "A feature extractor must accept a POS-tagged sentence `sentence`, the\n",
    "index `i` of a word in the sentence, and a tuple `history` containing\n",
    "the IOB tags that have already been assigned (presumably to earlier\n",
    "positions in the sentence). It must return a dictionary of the\n",
    "extracted features, where the keys are feature names and the values are the feature values. Because of the way the trainer works, the feature values have to be \"hashable\", which mostly means they can't be lists. If you want a list, turn it into a tuple with `tuple(my_list)`.\n",
    "\n",
    "Here is a very simple example, using just two features out of all the available information. The entire history is included so we can make sure the feature extractor is working right. (Are these useful features? Maybe, maybe not; the second half of the assignment is for figuring that out!)\n",
    "\n",
    "The name of this feature extractor and the docstring reflect the fact that this is just for testing the model code works correctly. When you make your own feature extractors, give them names that reflect their features, and docstrings that explain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_features(sentence, i, history):\n",
    "    \"\"\"dummy Chunker features designed to test the Chunker class for correctness\n",
    "        - the POS tag of the word\n",
    "        - the entire history of IOB tags so far\n",
    "            formatted as a tuple because it needs to be hashable\n",
    "    \"\"\" \n",
    "    word, pos = sentence[i]\n",
    "    return { \n",
    "        \"pos\": pos,\n",
    "        \"whole history\": tuple(history)\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here is a utility function for getting the `nltk.Tree` training data into the right format for the tagger, which is lists of ((word, POS), IOB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reformat_corpus_for_tagger(training_sentences):\n",
    "    \"\"\"\n",
    "    Given a corpus in nltk.Tree format, returns the corpus as a list of lists of tuples,\n",
    "    where each tuple ((word, POS), IOB) includes the word, its POS tag, and the IOB tag to be predicted.\n",
    "    @param training_sentences nltk.Tree list\n",
    "    \"\"\"\n",
    "    return [[((word, pos), iob) for (word, pos, iob) in tree2conlltags(sent)] for sent in training_sentences]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 1.A \n",
    "\n",
    "Write a function that takes a feature map and a list of training sentences as given in the corpus and returns a list of appropriate training data for the tagger. Call `reformat_corpus_for_tagger` on the sentences to get them into the right format.\n",
    "\n",
    "The tagger is trained not on the sentences themselves, but on the features extracted from the sentences, paired with the gold tags. A sentence should therefore become a list of (feature dictionary, IOB tag) pairs. \n",
    "\n",
    "Interestingly, even though the corpus is a list of lists, the tagger takes as input a flat list of the (feature dictionary, IOB tag) pairs.\n",
    "\n",
    "When you extract the features from the words in the sentence, be mindful of how feature functions work: for each word in a sentence, you need to give it the full current sentence in the form of (word, POS) pairs, the index of the word, and the history of IOB tags for the words before this word **in the sentence** (not in the whole corpus).\n",
    "\n",
    "Finally, when you pass the history to the feature map, make sure you're not passing it a pointer to a history that will keep changing. If you do this, all the words will end up with the history of the last word in the corpus. One way to handle this is to use `from copy import copy`, and then you can use, e.g., `copy(history)` to get a separate copy that won't change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "\n",
    "def create_training_data(feature_function, training_sentences):\n",
    "    \"\"\"\n",
    "    Creates training data from the corpus of training_sentences and the feature_function\n",
    "    :param feature_function: function that maps (untagged sentence, word index,\n",
    "     history) to a dict of features (from features.py)\n",
    "    :param training_sentences: training sentences as lists of nltk.Tree objects\n",
    "    :return: list of (dict, IOB tag) pairs\n",
    "    \"\"\"\n",
    "    # TODO reformat sentences to ((word, pos_tag), iob_tag) pairs\n",
    "    \n",
    "    # TODO turn the sentences into appropriate training data by finding their features\n",
    "    train_set = []\n",
    "    ...\n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this test to check that `create_training_data` is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "training_data = create_training_data(test_features, training[0:2])\n",
    "\n",
    "if training_data == [({'pos': 'Art', 'whole history': ()}, 'O'), ({'pos': 'N', 'whole history': ('O',)}, 'O'), ({'pos': 'Prep', 'whole history': ('O', 'O')}, 'O'), ({'pos': 'Art', 'whole history': ('O', 'O', 'O')}, 'O'), ({'pos': 'N', 'whole history': ('O', 'O', 'O', 'O')}, 'O'), ({'pos': 'V', 'whole history': ('O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Adv', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Adv', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Adj', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Adj', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Conj', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Art', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'N', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'V', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Adv', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'V', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Prep', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Art', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'N', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Conj', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'N', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'B-ORG'), ({'pos': 'V', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG')}, 'O'), ({'pos': 'Punc', 'whole history': ('O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O')}, 'O'), ({'pos': 'Prep', 'whole history': ()}, 'O'), ({'pos': 'Num', 'whole history': ('O',)}, 'O'), ({'pos': 'V', 'whole history': ('O', 'O')}, 'O'), ({'pos': 'Art', 'whole history': ('O', 'O', 'O')}, 'O'), ({'pos': 'Adj', 'whole history': ('O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Adj', 'whole history': ('O', 'O', 'O', 'O', 'O')}, 'B-MISC'), ({'pos': 'N', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC')}, 'O'), ({'pos': 'Art', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O')}, 'O'), ({'pos': 'N', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O')}, 'O'), ({'pos': 'Prep', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O')}, 'O'), ({'pos': 'Art', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'N', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O')}, 'B-MISC'), ({'pos': 'Pron', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC')}, 'O'), ({'pos': 'Art', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O')}, 'O'), ({'pos': 'N', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O')}, 'O'), ({'pos': 'Prep', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O')}, 'O'), ({'pos': 'Pron', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'N', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'V', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'V', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'V', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O'), ({'pos': 'Punc', 'whole history': ('O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O')}, 'O')]:\n",
    "    print(\"\\nTraining data is formated correctly\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n** Training data isn't formatted correctly!** Possible hints below\")\n",
    "    print(f\"Type is {type(training_data)} and should be <class 'list'>\")\n",
    "    if type(training_data) == list and len(training_data) > 0:\n",
    "        print(f\"Type of list members is {type(training_data[0])} and should be <class 'tuple'>\")\n",
    "        if len(training_data) > 1:\n",
    "            print(f\"item 2 is:\\n{training_data[2]}\\nand should be:\\n({{'pos': 'Prep', 'whole history': ['O', 'O']}}, 'O')\") \n",
    "\n",
    "            \n",
    "# If you can't tell what's wrong, try printing the whole thing\n",
    "# print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 1.B.  Complete the Chunker module\n",
    "\n",
    "Finish the following module by completing the `_ConsecutiveNPChunkTagger` class. Follow the `#TODO` instructions to complete the `__init__` method and transform your `create_training_data` function into a class method.\n",
    "\n",
    "**Warning**: Don't store the training data itself inside the class; this will make it impossible to pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FILE: incomplete_custom_chunker.py AS GIVEN IN ASSIGNMENT\n",
    "\n",
    "Based on code from http://www.nltk.org/book/ch07.html#code-classifier-chunker\n",
    "\n",
    "Authors: Alexis Dimitriadis, Meaghan Fowlie, and #TODO you!\n",
    "\n",
    "Use ConsecutiveNPChunker to train and use a classifier\n",
    "\n",
    "Treat _ConsecutiveNPChunkTagger as private: do not use it directly; it is called by ConsecutiveNPChunker\n",
    "\n",
    "\"\"\"\n",
    "from abc import ABC\n",
    "\n",
    "import nltk\n",
    "from nltk.chunk.util import conlltags2tree, tree2conlltags\n",
    "\n",
    "# If numpy is absent, nltk fails with a very confusing error.\n",
    "# We avoid problems by checking directly\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    print(\"You need to download and install numpy!!!\")\n",
    "    raise\n",
    "\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI, ABC):\n",
    "    \"\"\"\n",
    "    Trained classifier for NER\n",
    "    Classifier Input: a POS-tagged sentence -- (word, POS) list\n",
    "    Classifier Output: an IOB-tagged sentence -- ((word, POS), IOB) list\n",
    "    Attributes:\n",
    "        tagger: a _ConsecutiveNPChunkTagger object, trained on the feature map\n",
    "                and training set given to __init__\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_function, training_sentences, algorithm=\"NaiveBayes\", verbose=0):\n",
    "        \"\"\"\n",
    "        Train a classifier on chunked data in Tree format.\n",
    "        :param feature_function: The function that will compute features for each\n",
    "         word in a sentence. \n",
    "        :param training_sentences: A list of sentences in chunked (Tree) format.\n",
    "        :param algorithm: str: which classifier to use \n",
    "            (default NaiveBayes; other possibilities IIS, GIS, and DecisionTree)\n",
    "        :param verbose: int: how much to print during training (default 0, meaning nothing)\n",
    "        \"\"\"        \n",
    "        \n",
    "        # train the tagger\n",
    "        self.tagger = _ConsecutiveNPChunkTagger(feature_function,\n",
    "                                                training_sentences,\n",
    "                                                algorithm=algorithm,\n",
    "                                                verbose=verbose)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        \"\"\"\n",
    "        tag a sentence with IOB tags and return a tree\n",
    "        :param sentence: list of (word, POS) pairs\n",
    "        :return: Conll tree\n",
    "        \"\"\"\n",
    "        tagged_sent = self.tagger.tag(sentence)\n",
    "        # return to conll format\n",
    "        conll_tags = [(word, pos, iob) for ((word, pos), iob) in tagged_sent]\n",
    "        return conlltags2tree(conll_tags)\n",
    "\n",
    "    def explain(self):\n",
    "        \"\"\"Print the docstring of our feature extraction function\"\"\"\n",
    "        print(\"Algorithm:\", self.tagger.algorithm)\n",
    "        # Print the feature map's doc string:\n",
    "        print(self.tagger.feature_function.__doc__)\n",
    "\n",
    "    def show_most_informative_features(self, n=10):\n",
    "        \"\"\"\n",
    "        Call our classifier's `show_most_informative_features()` function.\n",
    "        :param n : int: the number of features to print (default 10)\n",
    "        \"\"\"\n",
    "        self.tagger.classifier.show_most_informative_features(n)\n",
    "        \n",
    "    def tag_corpus_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        tags a sentence in nltk.Tree form\n",
    "        :param sentence: nltk.Tree formated sentence, as in the corpora\n",
    "        :return tagged sentence as ((word, POS), IOB) pairs,\n",
    "                where IOB are the tags predicted by the model\n",
    "        \"\"\"\n",
    "        # turn the sentence into a unary list,\n",
    "        # use reformat_corpus_for_tagger,\n",
    "        # and untag the sentence\n",
    "        s = nltk.tag.untag(self.tagger.reformat_corpus_for_tagger([sentence])[0])\n",
    "        \n",
    "        # use the trained tagger to re-tag the sentence\n",
    "        return list(self.tagger.tag(s))\n",
    "    \n",
    "    def compare_output_to_gold(self, sentence):\n",
    "        \"\"\"\n",
    "        tags a sentence from the corpus and prints out a word-by-word comparison with the gold data\n",
    "        :param sentence: a sentence in nltk.Tree form, as in the corpora\n",
    "        \"\"\"\n",
    "        gold = self.tagger.reformat_corpus_for_tagger([sentence])[0]\n",
    "        tagged = self.tag_corpus_sentence(sentence)\n",
    "        print(\"gold\")\n",
    "        print(\"tagged\\n\")\n",
    "        for i in range(len(gold)):\n",
    "            print(gold[i])\n",
    "            print(tagged[i], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    \"\"\"This class is not meant to be\n",
    "    used directly: Use ConsecutiveNPChunker instead.\n",
    "    Attributes:\n",
    "        feature_function: map from\n",
    "                    (sentence, word index, history of features assigned so far)\n",
    "                    to dict of feature name: feature value.\n",
    "                    Imported from features.py.\n",
    "        train_set: list of (feature dict, IOB tag) pairs\n",
    "        classifier: nltk.NaiveBayesClassifier trained on training_sentences (default)\n",
    "        algorithm: str: name of the algorithm for reporting\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_function, training_sentences, algorithm=\"NaiveBayes\", verbose=0):\n",
    "        \"\"\"\n",
    "        Initialises and trains a tagger using the given features\n",
    "         and training sentences\n",
    "        :param feature_function: function that maps (untagged sentence, word index,\n",
    "         history) to a dict of features (from features.py)\n",
    "        :param training_sentences  : training sentences as list of\n",
    "                            ((word, pos_tag), iob_tag) pairs\n",
    "        :param algorithm: str:  which training algorithm to use. Default NaiveBayes.\n",
    "                                Other options are IIS, GIS, and DecisionTree.\n",
    "        :param verbose: int: IIS and GIS only: how much to print during training (0 = nothing)\n",
    "        \"\"\"\n",
    "\n",
    "        self.train_set = []  # initialise self.train_set\n",
    "\n",
    "        # TODO: store the feature_function parameter as self.feature_function\n",
    "        # TODO: call self.create_training_data on training_sentences\n",
    "        # TODO: check that algorithm is one of \"NaiveBayes\", \"DecisionTree\", \"IIS\", and \"GIS\"\n",
    "        # and raise an error if it's not\n",
    "\n",
    "\n",
    "        # set and train the classifier\n",
    "        if algorithm == \"NaiveBayes\":\n",
    "            self.classifier = nltk.NaiveBayesClassifier.train(self.train_set)\n",
    "            self.algorithm = \"Naive Bayes\"\n",
    "        elif algorithm == \"DecisionTree\":\n",
    "            self.classifier = nltk.DecisionTreeClassifier.train(self.train_set)\n",
    "            self.algorithm = \"Decision Tree\"\n",
    "        else:\n",
    "            self.classifier = nltk.MaxentClassifier.train(\n",
    "                self.train_set, algorithm=algorithm, trace=verbose)\n",
    "            self.algorithm = f\"Maximum Entropy with {algorithm}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def reformat_corpus_for_tagger(training_sentences):\n",
    "        \"\"\"\n",
    "        Given a corpus in nltk.Tree list format, returns the corpus as a list of lists of tuples,\n",
    "        where each tuple ((word, POS), IOB) includes the word, its POS tag, and the IOB tag to be predicted.\n",
    "        :param training_sentences nltk.Tree list of IOB-tagged sentences\n",
    "        \"\"\"\n",
    "        return [[((word, pos), iob) for (word, pos, iob) in tree2conlltags(sent)] for sent in training_sentences]\n",
    "\n",
    "    def create_training_data(self, training_sentences):\n",
    "        \"\"\"\n",
    "        Creates training data from the corpus of training_sentences and self.feature_function\n",
    "        stores a list of (dict, IOB tag) pairs as self.train_set\n",
    "\n",
    "        :param training_sentences: list of nltk.Trees with IOB tags\n",
    "\n",
    "        TODO make your function into a method that\n",
    "            uses the stored self.feature_function,\n",
    "            calls self.reformat_corpus_for_tagger on training_sentences,\n",
    "            and stores the training data as self.train_set\n",
    "            (and update this comment!)\n",
    "        \"\"\"\n",
    "        # TODO reformat sentences to ((word, pos_tag), iob_tag) pairs\n",
    "        # TODO turn the sentences into appropriate training data by finding their features\n",
    "        # TODO store them in self.train_set\n",
    "        ...\n",
    "\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        \"\"\"\n",
    "        uses the trained classifier to tag a sentence\n",
    "        :param sentence: list of (word, pos_tag) pairs\n",
    "        :return: list of ((word, pos_tag), IOB_tag) pairs\n",
    "        \"\"\"\n",
    "        history = []\n",
    "        for i in range(len(sentence)):\n",
    "            # extract the features\n",
    "            feature_dict = self.feature_function(sentence, i, history)\n",
    "            # tag the sentence\n",
    "            tag = self.classifier.classify(feature_dict)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of methods are built into this class that you might find helpful for your work. For instance, `ConsecutiveNPChunker.compare_output_to_gold` tags a sentence from the corpus and prints out a word-by-word comparison with the gold data.\n",
    "\n",
    "You can look at all their docstrings at once with `help`. Notice that some a inherited from their parent classes, `nltk.ChunkParserI` and `nltk.TaggerI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ConsecutiveNPChunker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(_ConsecutiveNPChunkTagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once we have our feature function, we can train a recognizer for the\n",
    "Dutch CONLL corpus as shown below. This classifier uses a Naive Bayes training algorithm.\n",
    "\n",
    "It sometimes takes a long time to train a recognizer so we demonstrate here with a tiny\n",
    "training set. Unsurprisingly,\n",
    "it's too small for the chunker to do anything useful with novel test\n",
    "data. Use larger datasets judiciously: Very short training sets are\n",
    "fine for checking if your code runs or crashes, but to find out if a new\n",
    "feature improves accuracy, you need to train on the entire dataset--\n",
    "or at least a substantial portion (several thousand sentences).\n",
    "\n",
    "If your code is too slow for anything but trivial datasets, figure out\n",
    "what is slowing it down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment this import statement if you want to import from a file instead of using the one in this Notebook\n",
    "# from custom_chunker import ConsecutiveNPChunker\n",
    "\n",
    "# train your model on the first 100 sentences\n",
    "training = conll.chunked_sents(\"ned.train\")[:100]\n",
    "\n",
    "# train a model on 100 training items\n",
    "test_nl_NER = ConsecutiveNPChunker(test_features, training)\n",
    "\n",
    "# Note: if you get ValueError: A ELE probability distribution must have at least one bin, it's probably because you haven't finished updating the code for the Chunker yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We evaluate our recognizer by calling its `accuracy` method.\n",
    "Evaluation is a lot faster than training, so we use the entire test\n",
    "set `ned.testa`.\n",
    "\n",
    "(Are you wondering why there's an `accuracy` method even though it's not in the code for `ConsecutiveNPChunker`? It's _inherited_ from its parent class, `nltk.ChunkParserI`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(test_nl_NER.accuracy(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `ConsecutiveNPChunker` class has some useful methods for development, including one that tags a sentence and displays the gold and predicted words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_nl_NER.compare_output_to_gold(testing[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Unsurprisingly, relying only on the part of speech and history is a very poor way\n",
    "to identify named entities. Our trivial recognizer trained on 100 sentences finds a negligible\n",
    "proportion of all named entities (0.2% recall). Of the chunks it marks\n",
    "as named entities, a small proportion (6.3%) are indeed named\n",
    "entities; the rest were marked incorrectly. It does, however, get most of the I/O/B tags right. (Why do you think this is?)\n",
    "\n",
    "When the whole training corpus is used, it finds no named entities at all. (Can you see why, with this feature set?)\n",
    "\n",
    "When you use the feature extractor `test_features` and train it on the same two data sets, you should also get these results. (If not, something is wrong.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "With 100 training sentences:\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  90.0%%\n",
    "    Precision:      6.3%%\n",
    "    Recall:         0.2%%\n",
    "    F-Measure:      0.4%%\n",
    "    \n",
    "With full training set:\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  90.1%%\n",
    "    Precision:      0.0%%\n",
    "    Recall:         0.0%%\n",
    "    F-Measure:      0.0%%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Step 2: Turn it into a program\n",
    "\n",
    "Training a non-trivial classifier is too time-consuming to keep\n",
    "entirely in a notebook. Prepare to work with python scripts (in Spyder\n",
    "or in your favorite editor), as follows:\n",
    "\n",
    "1. Save the `custom_chunker` module code above in a file named `custom_chunker.py`. Note that this code is also provided as `incomplete_custom_chunker.py`, so if you've been editing that file, just rename it to `custom_chunker.py`.\n",
    "\n",
    "2. Create a module `features.py` for your feature extractors. \n",
    "Put the definition of `test_features()` there as a starter. \n",
    "(You should later add, and use, additional functions.)\n",
    "\n",
    "3. You can now import both modules, or parts of them, for use in a\n",
    "Notebook or in other scripts. (Be careful if you're importing into a Notebook. If you edit the file, you may need to restart the kernel in order to properly re-import the edited module.) \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from custom_chunker import ConsecutiveNPChunker\n",
    "from features import test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_recognizer = ConsecutiveNPChunker(test_features, training)\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Pickling and unpickling successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have already seen how to pickle and reload a trained tagger.\n",
    "Working with a classifier is slightly more complicated, since its\n",
    "operation relies on code that we write and revise.  This requires some\n",
    "care to work correctly.\n",
    "\n",
    "It is important to understand that **pickling in python only stores\n",
    "data.** Pickled objects do not store python code for function or class\n",
    "definitions. To reload a pickled object, python must be able to find\n",
    "the definition of its class and of any functions the object refers to.\n",
    "\n",
    "1. During pickling, a record is made of the modules where the needed\n",
    "types and functions were defined.\n",
    "\n",
    "2. During unpickling, the types and functions are imported from the\n",
    "recorded modules and used with the reloaded objects.\n",
    "\n",
    "This is a bit of behind-the-screens magic, so you must take some care\n",
    "to avoid problems:\n",
    "\n",
    "2.  After you store a pickled object, you should not modify the\n",
    "functions and classes it depends on (i.e., `ConsecutiveNPChunker` and\n",
    "the feature extraction function it uses). **If you modify your feature \n",
    "function after pickling a model, the model will become invalid.**\n",
    "Your model may or may not cause runtime errors, but the statistics \n",
    "will be incorrect and you'll have to train and pickle a new\n",
    "version. Use a different name for each version of your feature extraction \n",
    "function (`chunkfeatures_2`, `big_features`, or whatever), so \n",
    "that unpickled models can retrieve the right function later.\n",
    "\n",
    "* Code can only be found in *named* modules, but the main script does\n",
    "not count as a regular named module (its name is always just\n",
    "`__main__`). If you define your feature function (e.g.,\n",
    "`chunkfeatures_1`) in your main script, pickle a model, and unpickle\n",
    "it from a _different_ script, python will not be able to find your\n",
    "function. The solution is simple:\n",
    "\n",
    "    * **All necessary classes and functions should be defined in\n",
    "modules (one or several), and *imported* into your main script.**\n",
    "\n",
    " The script that unpickles your model will then know where to find\n",
    "everything.\n",
    "\n",
    "\n",
    "\n",
    "**In short:** Use a different name for each new feature extraction\n",
    "function you define, and keep their definitions in modules, not in\n",
    "an `if __name__ == \"__main__\"` script section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Self-testing\n",
    "\n",
    "Here is a simple script that should be able to use your pickled\n",
    "tagger. Ensure that your code is compatible with it. If your code does\n",
    "not work with this script, **do not modify the script.** Fix your code\n",
    "so that it is compatible with the script. If your code does not work with this version of the testing script, it will not be compatible with our grading scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FILE: best_model_test.py\n",
    "Author: Alexis Dimitriadis\n",
    "\n",
    "Tests the functionality of a pickled model called best.pickle\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "from nltk.corpus import conll2002 as conll\n",
    "\n",
    "# best.pickle\n",
    "ner = pickle.load(open(\"best.pickle\", \"rb\"))\n",
    "\n",
    "# Usage 1: parse a list of sentences (with POS tags)\n",
    "tagzinnen = conll.tagged_sents(\"ned.train\")[1000:1050]\n",
    "ner.parse_sents(tagzinnen)\n",
    "\n",
    "# Usage 2: self-evaluate (on chunked sentences)\n",
    "chunkzinnen = conll.chunked_sents(\"ned.testa\")[1000:1500]\n",
    "\n",
    "print(ner.accuracy(chunkzinnen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Step 3: Write scripts to build and evaluate your models\n",
    "\n",
    "Write a script `build_models.py` that trains and pickles classifiers. \n",
    "\n",
    "Write a script `evaluate_models.py` that loads and evaluates your models. Make sure it prints at least: something that identifies the model/feature set, and the precision, recall, and F-measure for each model. You may also find you want to print more things; use your discretion.\n",
    "\n",
    "Give both scripts useful command-line interfaces that allow you to choose what to train and what to evaluate, and explain in a help message or a README file how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7. Step 4: Improve the model by improving the feature selection\n",
    "\n",
    "**In brief:** Choose suitable features to improve your classifier's\n",
    "performance.\n",
    "\n",
    "Now that you have a minimal working classifier, define better \n",
    "feature functions to improve your classifier's performance. Train with\n",
    "`ned.train`, and evaluate performance on the data in `ned.testa` only.\n",
    "\n",
    "You can start with including capitalization information about the\n",
    "current word; you can also experiment with features about the number\n",
    "of letters in the word, the tag that precedes or follows it, whether\n",
    "it follows a word that is already marked as part of a named entity\n",
    "(use the `history` vector), etc.\n",
    "\n",
    "You are also allowed to use (some) external resources. E.g., our list\n",
    "of Dutch proper names (from an earlier activity) might be helpful in\n",
    "recognizing whether some word is a person's name: Add a feature that\n",
    "tells you whether a word is the list of names (or perhaps in an abbreviated\n",
    "list containing just the most common names). You may also use any dataset\n",
    "that is provided by the nltk (available through `nltk.download`). Ask\n",
    "us first before using any other, external resources.\n",
    "\n",
    "These are only suggestions: Utilize your reading and your imagination\n",
    "to come up with more. Make sure you try **at least three** feature extractors. These three or more should have their feature extractors in/imported into `features.py`, be included in `evaluation_output.txt`, and be in your report.\n",
    "\n",
    "It is not necessary to report on (i.e. write about in your report) every version of the feature\n",
    "extractors you try out, but make sure you report on at least three.\n",
    "\n",
    "### `evaluation_output.txt`: a record of your experiments\n",
    "\n",
    "As you experiment with models, save the output of `evaluate_models` in a file `evaluation_output.txt`. The `evaluation_output.txt` and `features.py` that you hand in should be congruent: all feature functions in (or imported into) `features.py` should have output in `evaluation_output.txt` and any models in `evaluation_output.txt` should have their feature functions in (or imported into) `features.py`. Include an absolute minimum of 3 feature functions and an absolute minimum of 7 features altogether.  \n",
    "\n",
    "Make sure your `evaluate_models` script prints some information identifying the models. When you hand in your assignment, add a line to `evaluation_output.txt` identifying the model pickled as `best.pickle`, a line identifying the model pickles at `other.pickle`, and a line identifying your best model overall if you have an even better one with a different training algorithm (see Step 4.a below).\n",
    "\n",
    "### Step 4.a: OPTIONAL: improve other things about your model for bonus points\n",
    "\n",
    "If you're feeling ambitious, it's permitted to change other things; for example, you could trade out the learning algorithm for the MaxEnt GIS or IIS algorithm, or Decision Tree (available as options in the classifier class we built) and see if it's better.\n",
    "\n",
    "If you do anything that involves changing the provided code beyond what the assignment asks, make sure the default behaviour is the same as in the assignment, and be sure to document it. For example, if you add any new arguments to any methods, make sure they have default values. \n",
    "\n",
    "Please note that the Naive Bayes algorithm is much, much faster to train than the others, which is why it is the default in the class we built. It is not usually the best one, though. As such, we have different criteria for what counts as a good model depending on your algorithm. This is to keep the training manageable, especially if your computer is slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8. Step 5: Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train and pickle **two** models. Evaluate both on the data in `ned.testa`.\n",
    "\n",
    "One of these two models should be your **best Naive Bayes classifier** trained on the full `ned.train`. Call its pickle `best.pickle`.\n",
    "\n",
    "The other model is your choice, but there are a few rules:\n",
    "\n",
    "**Not allowed:**\n",
    "\n",
    "   * `test_features` as its feature extractor.\n",
    "   * Naive Bayes model trained on only part of the training data\n",
    "\n",
    "**Allowed:**\n",
    "\n",
    "   * Naive Bayes model trained on the whole `ned.train` and using a different feature extractor\n",
    "   * model using a different algorithm (e.g. MaxEnt), trained on any amount of the training data\n",
    "\n",
    "Call this one `other.pickle`.\n",
    "\n",
    "Precision and recall are discussed in [section 6.3.3][3.3] of the NLTK\n",
    "book. The section immediately below that presents the NLTK's\n",
    "[\"ConfusionMatrix( )\"][3.4] method, which is useful for identifying\n",
    "where your classifier makes mistakes. (Its use is optional).\n",
    "\n",
    "You may also find the nltk's [nltk.chunk.util.ChunkScore(\n",
    ")](https://www.nltk.org/api/nltk.chunk.util.html#nltk.chunk.util.ChunkScore)\n",
    "function useful (again, optional).\n",
    "\n",
    "[3.3]: http://www.nltk.org/book/ch06.html#precision-and-recall\n",
    "[3.4]:  http://www.nltk.org/book/ch06.html#confusion-matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**What performance should you aim for?** As a baseline, a very simple\n",
    "six-feature Naive Bayes classifier achieved about 42% precision and 57% recall,\n",
    "trained on the entire `ned.train` corpus and tested on `ned.testa`.\n",
    "Your solution should do at least this well.\n",
    "\n",
    "<pre>\n",
    "Algorithm: Naive Bayes\n",
    "Time to train: 6.433463261 seconds. (0.000407 per sentence)\n",
    "\n",
    "    6 features:\n",
    "        POS\n",
    "        word (string)\n",
    "        first letter is capital (boolean)\n",
    "        first letter of prev word is capital (boolean)\n",
    "        previous word\n",
    "        previous POS\n",
    "    This is the 6-feature one mentioned in the assignment\n",
    "    \n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  93.6%%\n",
    "    Precision:     42.0%%\n",
    "    Recall:        56.7%%\n",
    "    F-Measure:     48.2%%\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A more complex classifier based on NLTK's built-in NER function actually performs worse with the Naive Bayes algorithm, which is something to keep in mind as you explore: sometimes more features isn't better.\n",
    "\n",
    "<pre>\n",
    "\n",
    "Algorithm: Naive Bayes\n",
    "The feature set used by the NLTK's NER\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  90.4%%\n",
    "    Precision:     37.9%%\n",
    "    Recall:        51.7%%\n",
    "    F-Measure:     43.7%%\n",
    "\n",
    "</pre>\n",
    "\n",
    "With the IIS MaxEnt algorithm, it does pretty well, but requires overnight training. This is not expected of you.\n",
    "\n",
    "\n",
    "<pre>\n",
    "MODEL: NLTK feature set\n",
    "MaxEnt, IIS algorithm\n",
    "15 features, 15806 train sentences, 2895 testing\n",
    "Time to train: 9473 seconds (0.6s per sentence)\n",
    "\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  96.2%%\n",
    "    Precision:     73.8%%\n",
    "    Recall:        61.6%%\n",
    "    F-Measure:     67.2%%\n",
    "\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Your classifier will also be evaluated on sentences from outside the `train` and `testa` sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a script for checking your `other.pickle` model is correctly pickled. Make sure both `best_model_test.py` and `other_model_test.py` work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FILE: other_model_test.py\n",
    "Author: Alexis Dimitriadis\n",
    "\n",
    "Tests the functionality of a pickled model called other.pickle\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "from nltk.corpus import conll2002 as conll\n",
    "\n",
    "# other.pickle\n",
    "ner = pickle.load(open(\"other.pickle\", \"rb\"))\n",
    "\n",
    "# Usage 1: parse a list of sentences (with POS tags)\n",
    "tagzinnen = conll.tagged_sents(\"ned.train\")[1000:1050]\n",
    "ner.parse_sents(tagzinnen)\n",
    "\n",
    "# Usage 2: self-evaluate (on chunked sentences)\n",
    "chunkzinnen = conll.chunked_sents(\"ned.testa\")[1000:1500]\n",
    "\n",
    "print(ner.accuracy(chunkzinnen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Step 6: Report\n",
    "\n",
    "1-3 pages, **txt file, markdown file, or PDF only**, summarizing your results. (Cf. the results from step 4.)  You want this to be clear, walking your reader through your reasoning and information. Include:\n",
    "\n",
    "   * Your group number and member names\n",
    "   * Process:\n",
    "        * Any practical difficulties encountered in your group (e.g. a sick member, difficulty getting in contact, etc)\n",
    "        * Did you (try to) use Git? How was that?\n",
    "   * Models:\n",
    "        * Report **and briefly explain** the features in `features.py`\n",
    "            * Don't explain each feature extractor, just the features stored in the dictionaries\n",
    "            * You should have at least 7 features\n",
    "        * You should have a minimum of 3 feature extractor functions. For at least 3 models trained with at least 3 features extractors, report:\n",
    "            * precision, recall and F-scores\n",
    "            * features\n",
    "        * Compare the models stored in `best.pickle` and `other.pickle` (and any others you included)\n",
    "            * What are their precision, recall and F-scores?\n",
    "                * Comment on anything interesting here (e.g. precision and recall are very different, unexpected IOB accuracy...)\n",
    "            * How do the models differ in their design? (features, algorithm, anything else?)\n",
    "            * What do you think accounts for the difference in performance?\n",
    "        * Report about how much time was needed for training and for evaluation of your best model(s)\n",
    "            * Things like \"A few minutes/seconds\" or \"more than 10 minutes but less than an hour\" is accurate enough for fast; \"About n hours\" or \"overnight\" is fine for slow.\n",
    "        * Any additional explanations or observations about the task.\n",
    "   * If you ran into difficulties or did anything extra, this is where you should mention it.\n",
    "   * **Bonus**: You learned about the Naive Bayes algorithm in class. Do you have an idea about why it is so fast, and also so poor?\n",
    "\n",
    "Suggestions for making your report readable:\n",
    "\n",
    "   * Compare outcomes in tables so data can be viewed at a glance\n",
    "   * Put features in list form\n",
    "   * Section names\n",
    "   * At each juncture, say what you're talking about and why. Assume the reader hasn't read the report instructions and is just reading the report to find out about your results. (You can assume they know what the assignment is about.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 10. Step 7: Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Prepare to submit your code, organized in the following units (which\n",
    "may share some additional modules that you **also**\n",
    "submit).\n",
    "\n",
    "Double-check that the code and models you upload can run without\n",
    "modifications, and according to the instructions. Your code should\n",
    "contain no absolute paths or Windows-only paths with backslashes (`\\`).\n",
    "\n",
    "**Use good style**:\n",
    "\n",
    "* All imports should be at the beginning of the file\n",
    "* Name your functions and variables informatively\n",
    "* If you find yourself copying and pasting more than a couple of lines of code into multiple places, instead factor it out into a function.\n",
    "* Include **comments** for anything complex. A good rule of thumb is that if something is hard for you to write or explain, it might need a comment. \n",
    "* **Docstrings**: Make sure every **file, class, and function** has a docstring, and edit the docstrings and comments provided here if necessary, removing the TODOs and updating it to reflect anything about your code that isn't in the original docstring. \n",
    "\n",
    "In the file-level docstring, please include the authors of the file.\n",
    "\n",
    "### Zip the following files and hand in the zip file\n",
    "\n",
    "1. **Your python source files.** It should consist of the following\n",
    "standard files, plus any additional files you find it useful to\n",
    "create:\n",
    "\n",
    "   1. Your script `build_models.py` that trains and pickles the different classifiers you have developed. \n",
    "\n",
    "   2.  Your script `evaluate_models.py` that loads and evaluates your models. Ensure that the output of your evaluation script is a comprehensible report, not just raw numbers. E.g., print out a line identifying the classifier that you are about to calculate the `accuracy` of, and add some blank lines or other visual structure.\n",
    "    \n",
    "   3. The module `features.py` that defines, or imports from other modules, one or more feature extractor functions (whatever you find worth reporting on). \n",
    "    \n",
    "   4. The file `evaluation_output.txt`, with the saved output of the evaluation. This should include outputs for any models you mention in the report, even if they're not your best. We expect to see at least a few here. Add a line of text identifying the model saved as `best.pickle`.\n",
    "    \n",
    "   5. Any additional files you require, including external data (e.g., a list of names).\n",
    "    \n",
    "   6. **if needed:** Your version of `best_model_test.py` and `other_model_test.py` if you needed to make **any** changes to get it to run. (See below)\n",
    "\n",
    "2. **The pickled models of your best Naive Bayes classifier and a second classifier.** Please name them `best.pickle` and `other.pickle` respectively, and **test** that they can be reloaded and work correctly with the simple scripts `best_model_test.py` and `other_model_test.py` (see above). If you needed to make **any** changes to `model_test.py` to get it to run, be sure to include your version in the upload. **If you need to do this, it will cost you a few points**<p/>\n",
    "\n",
    "3. **Optional**: Any additional pickled model you want to share; e.g. if you trained with different algorithm\n",
    "\n",
    "4. **Report**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 11. Practicalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* There will be a small grade bonus (and bragging rights!) for the three groups that achieve the highest F-scores on new data with a Naive Bayes classifier, and bragging rights (only) for the 3 groups with the best models overall, if they're not Naive Bayes.\n",
    "\n",
    "* Your work should be your own. Cite external sources etc. with a comment or in the docstring if local in the code. Add a citation to your report for any external source you use.  Do not ask for help in any forum. DO NOT USE ANY LANGUAGE MODELS, including GPT and GitHub Copilot. (These can be useful tools for later, once you've learned the basics, but you may not use them for any assignments in this class this year.)\n",
    "\n",
    "*  You can freely use any part of the `conll` module itself and any\n",
    "other parts of the NLTK (except, obviously, the build-in NER!), including utility routines for evaluation, etc. You may use standard python libraries, and all libraries included with\n",
    "Anaconda; but not libraries that must be separately downloaded, except with prior approval. If in doubt,\n",
    "ask us.\n",
    "\n",
    "* Every member of your group should at least mostly understand all of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
